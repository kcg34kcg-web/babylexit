import os
import time
import threading
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager

# API KÃ¼tÃ¼phaneleri
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# AI ve VeritabanÄ±
from supabase import create_client, Client
from dotenv import load_dotenv
import pdfplumber
import pytesseract
from PIL import Image
from io import BytesIO
from sentence_transformers import SentenceTransformer

# --- KATMANLAR ---
from layers.guard import GuardLayer         # Katman 0: GÃ¼venlik
from layers.router import SemanticRouter    # Katman 1: YÃ¶nlendirme
from layers.rag import InternalRAGAgent     # Katman 2: Hukuk UzmanÄ±

# .env yÃ¼kle
load_dotenv()

# --- KONFIGÃœRASYON ---
SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
MODEL_NAME = 'BAAI/bge-m3'

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Hata: .env eksik veya hatalÄ±.")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Global DeÄŸiÅŸkenler
embed_model = None  # Yerel BGE-M3 Modeli (Dosya iÅŸleme ve RAG aramasÄ± iÃ§in)
guard = None        # GÃ¼venlik KatmanÄ±
router = None       # YÃ¶nlendirici (Gemini)
rag_agent = None    # Hukuk UzmanÄ± (RAG)

# --- MODEL YÃœKLEME (Yerel) ---
print(f"ğŸ“¥ Yerel AI Modeli YÃ¼kleniyor (CPU): {MODEL_NAME} ...")
try:
    embed_model = SentenceTransformer(MODEL_NAME, device='cpu')
    print("âœ… Yerel Embedding Modeli HazÄ±r!")
except Exception as e:
    print(f"âŒ Model HatasÄ±: {e}")
    exit(1)

# --- YARDIMCI FONKSÄ°YONLAR ---
def get_local_embedding(text: str) -> List[float]:
    """BGE-M3 ile 1024 boyutlu vektÃ¶r Ã¼retir."""
    try:
        embedding = embed_model.encode(text, normalize_embeddings=True)
        return embedding.tolist()
    except Exception as e:
        print(f"Embedding HatasÄ±: {e}")
        return []

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    chunks = []
    start = 0
    text_len = len(text)
    while start < text_len:
        end = start + chunk_size
        if end < text_len:
            last_space = text.rfind(' ', start, end)
            if last_space != -1 and last_space > start: end = last_space
        chunk = text[start:end].strip()
        if chunk: chunks.append(chunk)
        start = end - overlap
    return chunks

# --- WORKER (Dosya Ä°ÅŸleme - DeÄŸiÅŸmedi) ---
def process_queue_item(job):
    """Kuyruktaki dosyayÄ± iÅŸler ve vektÃ¶r veritabanÄ±na kaydeder."""
    try:
        job_id = job['id']
        file_path = job['file_path']
        print(f"ğŸ”„ Worker Ä°ÅŸliyor: {file_path}")
        
        supabase.table('file_processing_queue').update({'status': 'processing'}).eq('id', job_id).execute()
        
        res = supabase.storage.from_('raw_uploads').download(file_path)
        file_bytes = res
        
        text = ""
        ftype = job.get('file_type', '').lower()
        
        if 'pdf' in ftype:
            with pdfplumber.open(BytesIO(file_bytes)) as pdf:
                for page in pdf.pages:
                    extracted = page.extract_text()
                    if extracted: text += extracted + "\n"
        elif 'image' in ftype:
            img = Image.open(BytesIO(file_bytes))
            text = pytesseract.image_to_string(img, lang='tur+eng')
        else:
            text = file_bytes.decode('utf-8', errors='ignore')

        if len(text.strip()) < 10: 
            raise ValueError("Dosyadan anlamlÄ± veri okunamadÄ±.")

        chunks = chunk_text(text)
        docs = []
        for i, chunk in enumerate(chunks):
            vec = get_local_embedding(chunk)
            if vec:
                docs.append({
                    'content': chunk,
                    'metadata': {'source': file_path, 'user_id': job['user_id']},
                    'embedding': vec
                })
        
        if docs: 
            supabase.table('documents').insert(docs).execute()
        
        supabase.table('file_processing_queue').update({'status': 'completed'}).eq('id', job_id).execute()
        print(f"âœ… Worker TamamladÄ±: {file_path}")

    except Exception as e:
        print(f"âŒ Worker HatasÄ±: {e}")
        supabase.table('file_processing_queue').update({'status': 'failed', 'error_message': str(e)}).eq('id', job['id']).execute()

def run_worker_loop():
    print("ğŸ‘· Worker Thread BaÅŸladÄ±...")
    while True:
        try:
            res = supabase.table('file_processing_queue').select("*").eq('status', 'pending').limit(1).execute()
            if res.data:
                process_queue_item(res.data[0])
            else:
                time.sleep(2)
        except Exception as e:
            print(f"Worker Loop Error: {e}")
            time.sleep(5)

# --- LIFESPAN (BaÅŸlatma AyarlarÄ±) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global guard, router, rag_agent
    
    # 1. Guard (GÃ¼venlik)
    print("ğŸ›¡ï¸ GuardLayer BaÅŸlatÄ±lÄ±yor...")
    guard = GuardLayer()

    # 2. Router (Beyin)
    print("ğŸ§  Semantic Router BaÅŸlatÄ±lÄ±yor...")
    router = SemanticRouter()

    # 3. RAG Agent (Hukuk UzmanÄ±)
    print("âš–ï¸ RAG Agent BaÅŸlatÄ±lÄ±yor...")
    rag_agent = InternalRAGAgent(supabase)
    
    # 4. Worker (Arka Plan)
    worker_thread = threading.Thread(target=run_worker_loop, daemon=True)
    worker_thread.start()
    
    print("ğŸš€ BABYZLEXIT BACKEND HAZIR!")
    yield

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- REQUEST MODELLERÄ° ---
class EmbedRequest(BaseModel):
    text: str

class RouteRequest(BaseModel):
    query: str

# --- ENDPOINTS ---

@app.get("/")
def health_check():
    return {
        "status": "active", 
        "modules": ["Guard", "Router (Gemini)", "RAG Agent", "Local Embedding (BGE-M3)"]
    }

@app.post("/embed")
async def create_embedding(req: EmbedRequest):
    """Dosya yÃ¼kleme vb. iÃ§in sadece embedding dÃ¶ner."""
    vector = get_local_embedding(req.text)
    if not vector: raise HTTPException(status_code=500, detail="Embedding failed")
    return {"embedding": vector}

@app.post("/route")
async def route_query(req: RouteRequest):
    """
    ANA GÄ°RÄ°Å KAPISI:
    1. GÃ¼venlik KontrolÃ¼
    2. Rota Belirleme (Hukuk mu? Sohbet mi?)
    3. Gerekirse RAG Ã‡alÄ±ÅŸtÄ±rma (CevabÄ± Ã¼retme)
    """
    
    # 1. GÃ¼venlik
    security = await guard.analyze_input(req.query)
    if not security.is_safe:
         return {
             "action": "blocked",
             "response": f"GÃ¼venlik UyarÄ±sÄ±: {security.reason}",
             "confidence": 1.0
         }

    safe_query = security.refined_query or req.query
    
    # 2. YÃ¶nlendirme (Gemini DÃ¼ÅŸÃ¼nÃ¼yor)
    decision = await router.route(safe_query)
    
    # EÄŸer Router "Hukuk" veya "KarmaÅŸÄ±k" dediyse -> AvukatÄ± Ã‡aÄŸÄ±r (RAG)
    if decision.action == "route" and decision.target_layer in ["internal_rag", "hybrid_research"]:
        print(f"ğŸ”„ RAG KatmanÄ± Tetikleniyor: {safe_query}")
        
        # RAG iÃ§in Yerel Embedding Ãœret (Ã‡Ã¼nkÃ¼ veritabanÄ± BGE-M3 ile kayÄ±tlÄ±)
        rag_vector = get_local_embedding(safe_query)
        
        if rag_vector:
            # RAG Agent'a sor
            rag_result = await rag_agent.process(safe_query, rag_vector)
            
            # CevabÄ± Router sonucunun iÃ§ine gÃ¶mÃ¼yoruz
            # Frontend sadece 'cached_response' alanÄ±na bakarak cevabÄ± gÃ¶sterebilir
            decision.cached_response = rag_result["answer"]
            
            # KaynaklarÄ± reasoning'e ekle (Debug iÃ§in)
            if rag_result.get("sources"):
                decision.reasoning += f"\n[Referanslar: {', '.join(rag_result['sources'])}]"
        else:
            decision.reasoning += " (Embedding hatasÄ± nedeniyle RAG Ã§alÄ±ÅŸtÄ±rÄ±lamadÄ±)"

    return decision

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)