import os
import time
import threading
import asyncio
import logging
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager
from io import BytesIO

# API KÃ¼tÃ¼phaneleri
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# AI ve VeritabanÄ±
from supabase import create_client, Client
from dotenv import load_dotenv
import pdfplumber
import pytesseract          # <--- GERÄ° EKLENDÄ°
from PIL import Image       # <--- GERÄ° EKLENDÄ°
from sentence_transformers import SentenceTransformer

# --- LANGGRAPH ORKESTRASYONU ---
# Graph.py dosyasÄ±ndaki geliÅŸmiÅŸ akÄ±ÅŸÄ± import ediyoruz
try:
    from graph import start_analysis, app as graph_app
except ImportError:
    # Graph dosyasÄ± henÃ¼z yoksa hata vermemesi iÃ§in (Local test)
    start_analysis = None
    graph_app = None
    print("âš ï¸ UYARI: graph.py bulunamadÄ±. AI motoru sÄ±nÄ±rlÄ± modda Ã§alÄ±ÅŸacak.")

# .env yÃ¼kle
load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))

# --- KONFIGÃœRASYON ---
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
MODEL_NAME = 'BAAI/bge-m3'

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("BabyLexitMain")

if not SUPABASE_URL or not SUPABASE_KEY:
    logger.error("âŒ Hata: .env eksik veya hatalÄ±.")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Global DeÄŸiÅŸkenler
embed_model = None

# -----------------------------------------------------------------------------
# 1. DOSYA Ä°ÅžLEME VE EMBEDDING (OCR GÃœNCELLENDÄ°)
# -----------------------------------------------------------------------------

logger.info(f"ðŸ“¥ Yerel AI Modeli YÃ¼kleniyor (CPU): {MODEL_NAME} ...")
try:
    embed_model = SentenceTransformer(MODEL_NAME, device='cpu')
    logger.info("âœ… Yerel Embedding Modeli HazÄ±r!")
except Exception as e:
    logger.error(f"âŒ Model HatasÄ±: {e}")

def get_local_embedding(text: str) -> List[float]:
    try:
        embedding = embed_model.encode(text, normalize_embeddings=True)
        return embedding.tolist()
    except Exception as e:
        logger.error(f"Embedding HatasÄ±: {e}")
        return []

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        if chunk: chunks.append(chunk)
        start = end - overlap
    return chunks

def process_file_queue():
    """
    KullanÄ±cÄ±nÄ±n yÃ¼klediÄŸi dosyalarÄ± iÅŸler. (PDF + OCR Resim DesteÄŸi)
    """
    try:
        res = supabase.table('file_processing_queue').select("*").eq('status', 'pending').limit(1).execute()
        if not res.data: return False

        job = res.data[0]
        logger.info(f"ðŸ“‚ Dosya Ä°ÅŸleniyor: {job['file_path']}")
        
        supabase.table('file_processing_queue').update({'status': 'processing'}).eq('id', job['id']).execute()
        
        # DosyayÄ± Ä°ndir
        file_bytes = supabase.storage.from_('raw_uploads').download(job['file_path'])
        
        text = ""
        # Dosya tipini belirle (VeritabanÄ±ndan veya uzantÄ±dan)
        ftype = job.get('file_type', '').lower()
        if not ftype:
            ftype = job['file_path'].split('.')[-1].lower()
        
        logger.info(f"Tespit edilen dosya tipi: {ftype}")

        # --- DOSYA OKUMA MANTIÄžI ---
        if 'pdf' in ftype:
            # PDF Ä°ÅŸleme
            try:
                with pdfplumber.open(BytesIO(file_bytes)) as pdf:
                    for page in pdf.pages:
                        text += (page.extract_text() or "") + "\n"
            except Exception as pdf_err:
                logger.error(f"PDF Okuma HatasÄ±: {pdf_err}")
                
        elif ftype in ['jpg', 'jpeg', 'png', 'bmp', 'tiff']:
            # OCR Ä°ÅŸleme (Resimden YazÄ± Okuma)
            try:
                image = Image.open(BytesIO(file_bytes))
                # TÃ¼rkÃ§e dil desteÄŸi iÃ§in lang='tur' eklenebilir, varsayÄ±lan Ä°ngilizce+Genel'dir.
                # EÄŸer sunucuda tur paketi yoksa bu parametreyi kaldÄ±r: lang='tur'
                text = pytesseract.image_to_string(image) 
                logger.info("OCR iÅŸlemi tamamlandÄ±.")
            except Exception as ocr_err:
                logger.error(f"OCR HatasÄ± (Tesseract yÃ¼klÃ¼ mÃ¼?): {ocr_err}")
                raise ValueError("Resim iÅŸlenemedi. OCR motoru hatasÄ±.")
                
        else:
            # DÃ¼z Metin
            text = file_bytes.decode('utf-8', errors='ignore')

        # Ä°Ã§erik KontrolÃ¼
        if len(text.strip()) < 10: 
            raise ValueError(f"Dosyadan anlamlÄ± metin Ã§Ä±karÄ±lamadÄ± (Uzunluk: {len(text)})")

        # ParÃ§ala ve Kaydet
        chunks = chunk_text(text)
        docs = []
        for chunk in chunks:
            vec = get_local_embedding(chunk)
            if vec:
                docs.append({
                    'content': chunk,
                    'metadata': {'source': job['file_path'], 'user_id': job['user_id']},
                    'embedding': vec
                })
        
        if docs: supabase.table('documents').insert(docs).execute()
        
        supabase.table('file_processing_queue').update({'status': 'completed'}).eq('id', job['id']).execute()
        logger.info(f"âœ… Dosya TamamlandÄ±: {job['file_path']}")
        return True

    except Exception as e:
        logger.error(f"âŒ Dosya HatasÄ±: {e}")
        if 'job' in locals():
            supabase.table('file_processing_queue').update({'status': 'failed', 'error_message': str(e)}).eq('id', job['id']).execute()
        return False

# -----------------------------------------------------------------------------
# 2. ASYNC HELPER & QUESTION WORKER (LANGGRAPH)
# -----------------------------------------------------------------------------

def run_async(coro):
    """Senkron thread iÃ§inde Asenkron fonksiyon Ã§alÄ±ÅŸtÄ±rmak iÃ§in wrapper."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    if loop.is_running():
        return asyncio.run_coroutine_threadsafe(coro, loop).result()
    else:
        return loop.run_until_complete(coro)

def process_question_queue():
    """SÄ±radaki soruyu alÄ±r ve LangGraph OrkestratÃ¶rÃ¼ Ã¼zerinden geÃ§irir."""
    try:
        res = supabase.table('questions').select("*").eq('status', 'analyzing').limit(1).execute()
        
        if not res.data: 
            return False

        question = res.data[0]
        logger.info(f"âš–ï¸ Soru Tespit Edildi: {question['id']}")

        if start_analysis:
            run_async(start_analysis(question['id']))
            return True
        else:
            logger.warning("Graph modÃ¼lÃ¼ yÃ¼klÃ¼ deÄŸil, soru iÅŸlenemiyor.")
            return False

    except Exception as e:
        logger.error(f"âŒ Soru Worker HatasÄ±: {e}")
        return False

# -----------------------------------------------------------------------------
# 3. ANA DÃ–NGÃœ VE API
# -----------------------------------------------------------------------------

def run_worker_loop():
    logger.info("ðŸ‘· Worker Thread BaÅŸladÄ± (Dosya[OCR] + LangGraph)...")
    while True:
        try:
            did_file = process_file_queue()
            did_question = process_question_queue()

            if not did_file and not did_question:
                time.sleep(2)
                
        except Exception as e:
            logger.error(f"Worker Loop Critical Error: {e}")
            time.sleep(5)

@asynccontextmanager
async def lifespan(app: FastAPI):
    worker_thread = threading.Thread(target=run_worker_loop, daemon=True)
    worker_thread.start()
    
    logger.info("ðŸš€ BABYZLEXIT AI ENGINE (OCR + LangGraph) HAZIR!")
    yield

app = FastAPI(title="BabyLexit AI Service", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API ENDPOINTS ---

class AnalysisRequest(BaseModel):
    question_id: str

@app.get("/")
def read_root():
    return {"status": "active", "engine": "LangGraph + Gemini 2.0 + OCR"}

@app.post("/analyze")
async def trigger_analysis(request: AnalysisRequest, background_tasks: BackgroundTasks):
    if not request.question_id:
        raise HTTPException(status_code=400, detail="Question ID required")
    
    if start_analysis:
        background_tasks.add_task(start_analysis, request.question_id)
        return {"status": "accepted", "message": "Analysis started immediately"}
    
    return {"status": "error", "message": "Analysis engine not ready"}

class ChatRequest(BaseModel):
    query: str

@app.post("/api/chat")
async def chat_endpoint(req: ChatRequest):
    if not graph_app:
        raise HTTPException(status_code=503, detail="AI Engine not ready")
    
    try:
        inputs = {
            "question_id": "api-request",
            "query": req.query,
            "safety_status": "unknown",
            "route": "internal",
            "final_report": "",
            "status": "processing"
        }
        result = await graph_app.ainvoke(inputs)
        return {
            "response": result.get("final_report"),
            "route_used": result.get("route"),
            "sources": {
                "rag": result.get("rag_result").dict() if result.get("rag_result") else None,
                "web": result.get("web_result").dict() if result.get("web_result") else None
            }
        }
    except Exception as e:
        logger.error(f"API Chat Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)